{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Sun Sep 29 17:17:32 2019\\n\\n@author: Aman Nirala\\n\\nGithub: http://www.github.com/amannirala13\\nLinkedIn: http://www.linkedin/in/amannirala13\\nFacebook: http://www.facebook.com/amannirala13\\nInstagram: http://www.instagram.com/amannirala13\\nTwitter: http://www.twitter.com/amannirala13\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Sep 29 17:17:32 2019\n",
    "\n",
    "@author: Aman Nirala\n",
    "\n",
    "Github: http://www.github.com/amannirala13\n",
    "LinkedIn: http://www.linkedin/in/amannirala13\n",
    "Facebook: http://www.facebook.com/amannirala13\n",
    "Instagram: http://www.instagram.com/amannirala13\n",
    "Twitter: http://www.twitter.com/amannirala13\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\" from sklearn.preprocessing import Imputer \"\"\"                                                #If using sklearn version -0.20\n",
    "from sklearn.impute import SimpleImputer as imp                      #If using Sklearn version 0.20+\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "# >>> Importing dataset\n",
    "dataset = pd.read_csv('data.csv')                    #reading data from the csv file\n",
    "\n",
    "#__Logging data__\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# >>> Diving the dataset to attributes and dependent variables\n",
    "X = dataset.iloc[:, :-1].values                  #storing the attributes\n",
    "Y = dataset.iloc[:, 3].values               #storing the value to be predicted or dependent variable\n",
    "\n",
    "#__Logging data__\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# >>> Handeling missing data\n",
    "X[:, 1:3] = imp(missing_values = np.nan , strategy = 'mean', verbose = 0).fit_transform(X[:, 1:3])          #If using Sklearn version 0.20+\n",
    "\n",
    "#If using sklearn version -0.20\n",
    "\"\"\"\n",
    "imputer = Imputer(missing_values = np.nan, strategy = 'mean', axis = 0)         # Initialising the Impute object (it calls __inti__ function of imputter)\n",
    "imputer = imputer.fit(X[:, 1:3])                          # Fitting the imputer value to Nan from column index 1 to 2 \n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])                        #Transforming the NaN positions in original dataset\n",
    "\"\"\"\n",
    "\n",
    "#__Logging data__\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 0.00000000e+00 0.00000000e+00 4.40000000e+01\n",
      "  7.20000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 2.70000000e+01\n",
      "  4.80000000e+04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 3.00000000e+01\n",
      "  5.40000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.80000000e+01\n",
      "  6.10000000e+04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+01\n",
      "  6.37777778e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.50000000e+01\n",
      "  5.80000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.87777778e+01\n",
      "  5.20000000e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 4.80000000e+01\n",
      "  7.90000000e+04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 5.00000000e+01\n",
      "  8.30000000e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.70000000e+01\n",
      "  6.70000000e+04]]\n",
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amann\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\users\\amann\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# >>> Encoding caterogical data\n",
    "label_encoder_X = LabelEncoder()                               #creates an instance of LabelEncoder\n",
    "X[:, 0] = label_encoder_X.fit_transform(X[:, 0])                      #Fits and transforms the LabelEncoder to the first column of X\n",
    "one_hot_encoder = OneHotEncoder(categorical_features = [0])                 #Creates an instance of OneHotEncoder to one hot encode the first column of X\n",
    "X = one_hot_encoder.fit_transform(X).toarray()                    #Fits and transforms X using OneHotEncoder to one hot encode the first column of X\n",
    "\n",
    "label_encoder_Y = LabelEncoder()                              #creates another instance of LabelEncoder\n",
    "Y = label_encoder_Y.fit_transform(Y)                             #Fit and transform the LabelEncoder to the the columns of Y\n",
    "\n",
    "#__Logging data__\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+01\n",
      "  6.37777778e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.70000000e+01\n",
      "  6.70000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 2.70000000e+01\n",
      "  4.80000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.87777778e+01\n",
      "  5.20000000e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 4.80000000e+01\n",
      "  7.90000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.80000000e+01\n",
      "  6.10000000e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 4.40000000e+01\n",
      "  7.20000000e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.50000000e+01\n",
      "  5.80000000e+04]]\n",
      "[1 1 1 0 1 0 0 1]\n",
      "[[0.0e+00 1.0e+00 0.0e+00 3.0e+01 5.4e+04]\n",
      " [0.0e+00 1.0e+00 0.0e+00 5.0e+01 8.3e+04]]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "# >>> Splitting the data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2 , random_state = 0)   #Splits the dataset into training and testing (Change the test_size according to the need and your dataset)\n",
    "\n",
    "#__Logging data__\n",
    "print(X_train)\n",
    "print(Y_train)\n",
    "print(X_test)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.          2.64575131 -0.77459667  0.26306757  0.12381479]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.25350148  0.46175632]\n",
      " [-1.         -0.37796447  1.29099445 -1.97539832 -1.53093341]\n",
      " [-1.         -0.37796447  1.29099445  0.05261351 -1.11141978]\n",
      " [ 1.         -0.37796447 -0.77459667  1.64058505  1.7202972 ]\n",
      " [-1.         -0.37796447  1.29099445 -0.0813118  -0.16751412]\n",
      " [ 1.         -0.37796447 -0.77459667  0.95182631  0.98614835]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.59788085 -0.48214934]]\n",
      "[[-1.          2.64575131 -0.77459667 -1.45882927 -0.90166297]\n",
      " [-1.          2.64575131 -0.77459667  1.98496442  2.13981082]]\n"
     ]
    }
   ],
   "source": [
    "# >>> Feature Scaling\n",
    "stdSc = StandardScaler()                              #Created an instance of StandardScaler\n",
    "X_train = stdSc.fit_transform(X_train)                         #Scaling by fitting the X_train (training) dataset \n",
    "X_test = stdSc.transform(X_test)                         #Scaling X_test (testing) dataset\n",
    "\n",
    "#__Logging data__\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
